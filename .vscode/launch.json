{
    "version": "0.2.0",
    "configurations": [
        {
            "name": "Python: finetune kv",
            "type": "python",
            "request": "launch",
            "program": "finetune.py",
            "console": "integratedTerminal",
            "justMyCode": true,
            "args": [
                "--base_model",
                "'decapoda-research/llama-7b-hf'",
                "--data_path",
                "'math_data.json' ",
                "--output_dir",
                "'./trained_models/llama-kv'",
                "--batch_size",
                "4",
                "--micro_batch_size",
                "4",
                "--num_epochs",
                "3",
                "--learning_rate",
                "3e-4",
                "--cutoff_len",
                "256",
                "--val_set_size",
                "120",
                "--adapter_name",
                "bottleneck",
                "--use_global_kv_adapter",
                "--codebook_nums",
                "256",
                "--num_memories",
                "16",
            ]
        },
        {
            "name": "Python: finetune lora",
            "type": "python",
            "request": "launch",
            "program": "finetune.py",
            "console": "integratedTerminal",
            "justMyCode": true,
            "args": [
                "--base_model",
                "'decapoda-research/llama-7b-hf'",
                "--data_path",
                "'math_data.json' ",
                "--output_dir",
                "'./trained_models/llama-lora'",
                "--batch_size",
                "4",
                "--micro_batch_size",
                "4",
                "--num_epochs",
                "3",
                "--learning_rate",
                "3e-4",
                "--cutoff_len",
                "256",
                "--val_set_size",
                "120",
                "--adapter_name",
                "lora",
            ],
            "env": {
                "CUDA_VISIBLE_DEVICES": "7"
            },
        },
        {
            "name": "Python: finetune kvmlora",
            "type": "python",
            "request": "launch",
            "program": "finetune.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "--base_model",
                "'decapoda-research/llama-7b-hf'",
                "--data_path",
                "'math_data.json' ",
                "--output_dir",
                "'./trained_models/llama-kvmlora'",
                "--batch_size",
                "4",
                "--micro_batch_size",
                "4",
                "--num_epochs",
                "3",
                "--learning_rate",
                "3e-4",
                "--cutoff_len",
                "256",
                "--val_set_size",
                "120",
                "--adapter_name",
                "kvmlora",
                "--codebook_nums",
                "2",
                "--num_memories",
                "4",
            ],
            "env": {
                "CUDA_VISIBLE_DEVICES": "7"
            },
        },
        {
            "name": "Python: finetune-prototypelora",
            "type": "python",
            "request": "launch",
            "program": "finetune.py",
            "console": "integratedTerminal",
            "justMyCode": true,
            "args": [
                "--base_model",
                "'decapoda-research/llama-7b-hf'",
                "--data_path",
                "'math_data.json' ",
                "--output_dir",
                "'./trained_models/llama-prototpye'",
                "--batch_size",
                "4",
                "--micro_batch_size",
                "4",
                "--num_epochs",
                "3",
                "--learning_rate",
                "3e-4",
                "--cutoff_len",
                "256",
                "--val_set_size",
                "120",
                "--adapter_name",
                "prototypelora",
            ],
            "env": {
                "CUDA_VISIBLE_DEVICES": "7"
            },
        },
        {
            "name": "Python: generate gradio interface",
            "type": "python",
            "request": "launch",
            "program": "generate.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "--base_model",
                "decapoda-research/llama-7b-hf",
                "--lora_weights",
                "trained_models/llama-kvlora/checkpoint-2200"
            ],
            "env": {
                "CUDA_VISIBLE_DEVICES": "0"
            }
        },
        {
            "name": "Python: evaluate",
            "type": "python",
            "request": "launch",
            "program": "evaluate.py",
            "console": "integratedTerminal",
            "justMyCode": false,
            "args": [
                "--base_model",
                "yahma/llama-7b-hf",
                "--dataset",
                "AddSub",
                "--model",
                "LLaMA-7B",
                "--adapter",
                "kvmlora",
                "--lora_weights",
                "trained_models/yahmallama-kvmlora/"
            ],
            "env": {
                "CUDA_VISIBLE_DEVICES": "0"
            }
        }
    ]
}